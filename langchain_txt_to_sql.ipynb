{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* GA4 sample 데이터 스키마를 기반. 해당 테이블을 기준으로 프롬프트 질의 시 질의에 정답에 해당하는 sql query 문 생성\n",
    "\n",
    "### Process\n",
    "1. 문서의 내용을 읽는다.\n",
    "2. 문서를 쪼갠다 (chunking)\n",
    "3. 문서를 임베딩 해서 벡터 데이터베이스 저장 (Pinecone)\n",
    "4. 질문이 있을 때 벡터 데이터베이스에서 유사도를 검색\n",
    "5. 유사도 검색으로 가져온 문서를 LLM에 질문과 같이 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a202304035/.pyenv/versions/3.11.0/envs/llm_prototype/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "embedding = OpenAIEmbeddings(model = 'text-embedding-3-large')\n",
    "llm = ChatOpenAI(model ='gpt-4o')\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, # chunk 하나가 가질 수 있는 token 수\n",
    "    # chunk_overlap = 50 # chunk 간 token 을 겹치게 하는 범위 (유사도 검색의 성능을 더 올림)\n",
    ")\n",
    "\n",
    "loader = Docx2txtLoader('./GA4_schema_markdown.docx')\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GA4] BigQuery Export schema\n",
      "\n",
      "This article explains the format and schema of the Google Analytics 4 property data and the Google Analytics for Firebase data that is exported to BigQuery.\n",
      "\n",
      "Datasets\n",
      "\n",
      "For each Google Analytics 4 property and each Firebase project that is linked to BigQuery, a single dataset named \"analytics_<property_id>\" is added to your BigQuery project. Property ID refers to your Analytics Property ID, which you can find in the property settings for your Google Analytics 4 property, and in App Analytics Settings in Firebase. Each Google Analytics 4 property and each app for which BigQuery exporting is enabled will export its data to that single dataset.\n",
      "\n",
      "Tables\n",
      "\n",
      "Within each dataset, a table named events_YYYYMMDD is created each day if the Daily export option is enabled.\n"
     ]
    }
   ],
   "source": [
    "print(document_list[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "index_name = 'ga4-sql-index'\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "database = PineconeVectorStore.from_documents(document_list, embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '2024년 11월 1일 기준으로, device tpye 별로 unique user 의 수를 집계하는 쿼리를 작성해주세요.'\n",
    "query = 'event 가 발생한 날짜를 2020년 12월 한 달로 설정하고, device tpye 별로 전체 구분 가능한 user 의 수와 구매자의 수, 해당 기간 동안의 총 구매액 합계를 추출하는 쿼리를 작성해주세요.'\n",
    "retrieved_docs = database.similarity_search(query, k= 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6b3a39e4-9f2a-4afa-af57-61c6b6f82dc0', metadata={'source': './GA4_schema_markdown.docx'}, page_content='| Field name | Data type | Description |\\n| ecommerce.total_item_quantity | INTEGER | Total number of items in this event, which is the sum of items.quantity. |\\n| ecommerce.purchase_revenue_in_usd | FLOAT | Purchase revenue of this event, represented in USD with standard unit. Populated for purchase event only. |\\n| ecommerce.purchase_revenue | FLOAT | Purchase revenue of this event, represented in local currency with standard unit. Populated for purchase event only. |'),\n",
       " Document(id='dc8e1ea3-9816-4fbd-b8ee-fea52924367a', metadata={'source': './GA4_schema_markdown.docx'}, page_content='Tables\\n\\nWithin each dataset, a table named\\xa0events_YYYYMMDD\\xa0is created each day if the Daily export option is enabled.\\n\\nIf the Streaming export option is enabled, a table named\\xa0events_intraday_YYYYMMDD\\xa0is created. This table is populated continuously as events are recorded throughout the day. This table is deleted at the end of each day once\\xa0events_YYYYMMDD\\xa0is complete.\\n\\nNot all devices on which events are triggered send their data to Analytics on the same day the events are triggered. To account for this latency, Analytics will update the daily tables (events_YYYYMMDD) with events for those dates for up to three days after the dates of the events. Events will have the correct time stamp regardless of arriving late. Events that arrive after that three-day window are not recorded.\\n\\nIf you are using\\xa0BigQuery sandbox, there is no intraday import of events, and\\xa0additional limits apply.\\n\\nUpgrade from the sandbox\\xa0if you want intraday imports.\\n\\nColumns'),\n",
       " Document(id='afdefc46-8e95-4ac3-844e-9a99a8b3abd9', metadata={'source': './GA4_schema_markdown.docx'}, page_content='Tables\\n\\nWithin each dataset, a table named\\xa0events_YYYYMMDD\\xa0is created each day if the Daily export option is enabled.\\n\\nIf the Streaming export option is enabled, a table named\\xa0events_intraday_YYYYMMDD\\xa0is created. This table is populated continuously as events are recorded throughout the day. This table is deleted at the end of each day once\\xa0events_YYYYMMDD\\xa0is complete.\\n\\nNot all devices on which events are triggered send their data to Analytics on the same day the events are triggered. To account for this latency, Analytics will update the daily tables (events_YYYYMMDD) with events for those dates for up to three days after the dates of the events. Events will have the correct time stamp regardless of arriving late. Events that arrive after that three-day window are not recorded.\\n\\nIf you are using\\xa0BigQuery sandbox, there is no intraday import of events, and\\xa0additional limits apply.\\n\\nUpgrade from the sandbox\\xa0if you want intraday imports.\\n\\nColumns'),\n",
       " Document(id='1afb7561-ea8f-4147-98a9-4a0d02036f7f', metadata={'source': './GA4_schema_markdown.docx'}, page_content='| event_date | STRING | The date when the event was logged (YYYYMMDD format in the registered timezone of your app). |\\n| event_timestamp | INTEGER | The time (in microseconds, UTC) when the event was logged on the client. |\\n| event_previous_timestamp | INTEGER | The time (in microseconds, UTC) when the event was previously logged on the client. |\\n| event_name | STRING | The name of the event. |\\n| event_value_in_usd | FLOAT | The currency-converted value (in USD) of the event\\'s \"value\" parameter. |\\n| event_bundle_sequence_id | INTEGER | The sequential ID of the bundle in which these events were uploaded. |\\n| event_server_timestamp_offset | INTEGER | Timestamp offset between collection time and upload time in micros. |'),\n",
       " Document(id='0c5b77ac-2e53-4d85-89c4-1512544a2d0c', metadata={'source': './GA4_schema_markdown.docx'}, page_content='| event_date | event_timestamp | event_name | event_params.key | event_params_value.string_value |\\n| --- | --- | --- | --- | --- |\\n| 20220222 | 1643673600483790 | page_view | page_location | https://example.com |\\n|  |  |  | page_title | Home |\\n|  |  |  | medium | referral |\\n|  |  |  | source | google |\\n|  |  |  | page_referrer | https://www.google.com |\\n|  |  |  | <parameters...> | <values...> |\\n\\nThis event data is displayed as follows in the GA4 user interface.'),\n",
       " Document(id='7e7a186f-17dc-4885-afc8-50cc9f88feb0', metadata={'source': './GA4_schema_markdown.docx'}, page_content='| event_date | event_timestamp | event_name | event_params.key | event_params_value.string_value |\\n| --- | --- | --- | --- | --- |\\n| 20220222 | 1643673600483790 | page_view | page_location | https://example.com |\\n|  |  |  | page_title | Home |\\n|  |  |  | medium | referral |\\n|  |  |  | source | google |\\n|  |  |  | page_referrer | https://www.google.com |\\n|  |  |  | <parameters...> | <values...> |\\n\\nThis event data is displayed as follows in the GA4 user interface.'),\n",
       " Document(id='1947c40d-c423-4bc9-91a2-3a201cb48137', metadata={'source': './GA4_schema_markdown.docx'}, page_content='If you are using\\xa0BigQuery sandbox, there is no intraday import of events, and\\xa0additional limits apply.\\n\\nUpgrade from the sandbox\\xa0if you want intraday imports.\\n\\nColumns\\n\\nEach column in the\\xa0events_YYYYMMDD\\xa0table represents an event-specific parameter. Note that some parameters are nested within RECORDS, and some RECORDS such as\\xa0items\\xa0and\\xa0event_params\\xa0are repeatable. Table columns are described below.\\n\\nExpand all\\xa0Collapse all\\n\\nevent')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"[identity]\n",
    "- 당신은 데이터 과학자이자 데이터 분석가입니다. 주어진 데이터에 대한 정보를 바탕으로 데이터를 활용하고자 하는 사람들의 질문에 답할 수 있는 쿼리를 작성할 수 있습니다. \n",
    "- 특히 Bigquery 환경에서 필요한 테이블을 추출하는데 어려움이 없습니다.\n",
    "- [Context]를 참고해서 사용자의 질문에 답변해주세요.\n",
    "\n",
    "[Context]\n",
    "{retrieved_docs}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To address your request, we will create a BigQuery SQL query that extracts data for events happening in December 2020, grouped by device type. We will calculate the distinct number of users, the number of purchasers, and the total purchase revenue for that period. Here's how you can structure the query:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "  device.type AS device_type,\n",
      "  COUNT(DISTINCT user_pseudo_id) AS total_users,\n",
      "  COUNT(DISTINCT CASE WHEN ecommerce.purchase_revenue IS NOT NULL THEN user_pseudo_id END) AS total_purchasers,\n",
      "  SUM(ecommerce.purchase_revenue_in_usd) AS total_purchase_revenue\n",
      "FROM\n",
      "  `your_project.your_dataset.events_*`\n",
      "WHERE\n",
      "  _TABLE_SUFFIX BETWEEN '20201201' AND '20201231' -- Filter for December 2020\n",
      "  AND event_name = 'purchase' -- Considering only purchase events for revenue\n",
      "GROUP BY\n",
      "  device_type\n",
      "ORDER BY\n",
      "  total_purchase_revenue DESC;\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **`device.type`**: Assumed field for device type. You might need to adjust based on your schema.\n",
      "- **`user_pseudo_id`**: A unique identifier for users.\n",
      "- **`ecommerce.purchase_revenue_in_usd`**: The field representing purchase revenue in USD.\n",
      "- **Filtering by date**: Uses `_TABLE_SUFFIX` to select tables for December 2020.\n",
      "- **Filtering by event**: The `event_name` is filtered to 'purchase' to consider only purchase-related events.\n",
      "- **Grouping and ordering**: Results are grouped by device type and ordered by total purchase revenue in descending order.\n",
      "\n",
      "Ensure to replace `your_project.your_dataset` with your actual BigQuery project and dataset names. Also, verify that the field names (like `device.type`) match those in your dataset schema.\n"
     ]
    }
   ],
   "source": [
    "print(ai_message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
